{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0670a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  combine demand\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = 'rawCSV/demand'\n",
    "output_path = 'rawCSV/combinedDatasets'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Initialize empty list to store dataframes\n",
    "all_data = []\n",
    "\n",
    "# Loop through years 2002-2025\n",
    "for year in range(2002, 2026):\n",
    "    file_path = f'{raw_data_path}/PUB_Demand_{year}.csv'\n",
    "    \n",
    "    try:\n",
    "        # Read CSV, skipping first 3 header rows\n",
    "        df = pd.read_csv(file_path, skiprows=3)\n",
    "        \n",
    "        # Remove any completely empty rows\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Remove any trailing empty columns\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        df = df.drop(columns=[\"Market Demand\"], errors=\"ignore\")\n",
    "        \n",
    "        # Add year column for reference\n",
    "        \n",
    "        all_data.append(df)\n",
    "        print(f\"✓ Successfully loaded {year}: {len(df)} rows\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {year}: {str(e)}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Clean the data\n",
    "    # Remove any rows where all demand columns are empty\n",
    "    combined_df = combined_df.dropna(subset=['Ontario Demand'], how='all')\n",
    "    \n",
    "    # Convert data types\n",
    "    combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "    combined_df['Hour'] = pd.to_numeric(combined_df['Hour'], errors='coerce')\n",
    "    # combined_df['Market Demand'] = pd.to_numeric(combined_df['Market Demand'], errors='coerce')\n",
    "    combined_df['Ontario Demand'] = pd.to_numeric(combined_df['Ontario Demand'], errors='coerce')\n",
    "    \n",
    "    # Sort by date and hour\n",
    "    combined_df = combined_df.sort_values(['Date', 'Hour']).reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = f'{output_path}/combined_demand_2002_2025.csv'\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Successfully combined {len(all_data)} files\")\n",
    "    print(f\"✓ Total rows: {len(combined_df):,}\")\n",
    "    print(f\"✓ Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "    print(f\"✓ Saved to: {output_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Display first and last few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(combined_df.head())\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(combined_df.tail())\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nData Info:\")\n",
    "    print(combined_df.info())\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(combined_df.describe())\n",
    "    \n",
    "else:\n",
    "    print(\"No data files were successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b8204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded 2003: 5880 rows\n",
      "✓ Successfully loaded 2004: 8784 rows\n",
      "✓ Successfully loaded 2005: 8760 rows\n",
      "✓ Successfully loaded 2006: 8760 rows\n",
      "✓ Successfully loaded 2007: 8760 rows\n",
      "✓ Successfully loaded 2008: 8784 rows\n",
      "✓ Successfully loaded 2009: 8760 rows\n",
      "✓ Successfully loaded 2010: 8760 rows\n",
      "✓ Successfully loaded 2011: 8760 rows\n",
      "✓ Successfully loaded 2012: 8784 rows\n",
      "✓ Successfully loaded 2013: 8760 rows\n",
      "✓ Successfully loaded 2014: 8760 rows\n",
      "✓ Successfully loaded 2015: 8760 rows\n",
      "✓ Successfully loaded 2016: 8784 rows\n",
      "✓ Successfully loaded 2017: 8760 rows\n",
      "✓ Successfully loaded 2018: 8760 rows\n",
      "✓ Successfully loaded 2019: 8760 rows\n",
      "✓ Successfully loaded 2020: 8784 rows\n",
      "✓ Successfully loaded 2021: 8760 rows\n",
      "✓ Successfully loaded 2022: 8760 rows\n",
      "✓ Successfully loaded 2023: 8760 rows\n",
      "✓ Successfully loaded 2024: 8784 rows\n",
      "✓ Successfully loaded 2025: 6935 rows\n",
      "\n",
      "============================================================\n",
      "✓ Successfully combined 23 files\n",
      "✓ Total rows: 196,919\n",
      "✓ Date range: 2003-05-01 00:00:00 to 2025-10-16 00:00:00\n",
      "✓ Saved to: rawCSV/combinedDatasets/combined_demandZonal_2003_2025.csv\n",
      "============================================================\n",
      "\n",
      "First 5 rows:\n",
      "        Date  Hour  Ontario Demand  Northwest  Northeast  Ottawa  East  \\\n",
      "0 2003-05-01     1           13702        809       1284     965   765   \n",
      "1 2003-05-01     2           13578        825       1283     923   752   \n",
      "2 2003-05-01     3           13411        834       1277     910   751   \n",
      "3 2003-05-01     4           13501        835       1277     922   758   \n",
      "4 2003-05-01     5           14010        847       1268     993   804   \n",
      "\n",
      "   Toronto  Essa  Bruce  Southwest  Niagara  West  Zone Total   Diff  \n",
      "0     4422   622     41       2729      617  1611       13865  163.0  \n",
      "1     4340   602     43       2731      615  1564       13678  100.0  \n",
      "2     4281   591     45       2696      596  1553       13534  123.0  \n",
      "3     4281   599     41       2724      609  1544       13590   89.0  \n",
      "4     4469   643     51       2842      579  1592       14088   78.0  \n",
      "\n",
      "Last 5 rows:\n",
      "             Date  Hour  Ontario Demand  Northwest  Northeast  Ottawa  East  \\\n",
      "196914 2025-10-16    20           16622        591       1278     986  1123   \n",
      "196915 2025-10-16    21           16204        581       1239     979  1044   \n",
      "196916 2025-10-16    22           15274        571       1247     900   979   \n",
      "196917 2025-10-16    23           14615        560       1233     830   902   \n",
      "196918 2025-10-16    24           13987        538       1210     781   847   \n",
      "\n",
      "        Toronto  Essa  Bruce  Southwest  Niagara  West  Zone Total   Diff  \n",
      "196914     5920  1163    107       3303      557  1768       16795  173.0  \n",
      "196915     5714  1110    105       3211      564  1727       16274   70.0  \n",
      "196916     5440  1037    102       3136      554  1659       15626  352.0  \n",
      "196917     5062   960     99       2981      552  1663       14841  226.0  \n",
      "196918     4788   904     96       2832      554  1634       14184  197.0  \n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 196919 entries, 0 to 196918\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   Date            196919 non-null  datetime64[ns]\n",
      " 1   Hour            196919 non-null  int64         \n",
      " 2   Ontario Demand  196919 non-null  int64         \n",
      " 3   Northwest       196919 non-null  int64         \n",
      " 4   Northeast       196919 non-null  int64         \n",
      " 5   Ottawa          196919 non-null  int64         \n",
      " 6   East            196919 non-null  int64         \n",
      " 7   Toronto         196919 non-null  int64         \n",
      " 8   Essa            196919 non-null  int64         \n",
      " 9   Bruce           196919 non-null  int64         \n",
      " 10  Southwest       196919 non-null  int64         \n",
      " 11  Niagara         196919 non-null  int64         \n",
      " 12  West            196919 non-null  int64         \n",
      " 13  Zone Total      196919 non-null  int64         \n",
      " 14  Diff            196790 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(13)\n",
      "memory usage: 22.5 MB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "                                Date           Hour  Ontario Demand  \\\n",
      "count                         196919  196919.000000   196919.000000   \n",
      "mean   2014-07-23 23:31:13.921764864      12.500058    16149.180892   \n",
      "min              2003-05-01 00:00:00       1.000000     2270.000000   \n",
      "25%              2008-12-11 00:00:00       7.000000    14246.000000   \n",
      "50%              2014-07-24 00:00:00      13.000000    16034.000000   \n",
      "75%              2020-03-05 00:00:00      18.500000    17895.000000   \n",
      "max              2025-10-16 00:00:00      24.000000    27005.000000   \n",
      "std                              NaN       6.922173     2547.745862   \n",
      "\n",
      "           Northwest      Northeast         Ottawa           East  \\\n",
      "count  196919.000000  196919.000000  196919.000000  196919.000000   \n",
      "mean      567.402480    1266.983613    1077.958313     984.617462   \n",
      "min         0.000000    -380.000000       0.000000       0.000000   \n",
      "25%       458.000000    1137.000000     885.000000     834.000000   \n",
      "50%       528.000000    1256.000000    1052.000000     984.000000   \n",
      "75%       632.000000    1384.000000    1249.000000    1135.000000   \n",
      "max      1149.000000    2026.000000    2508.000000    1858.000000   \n",
      "std       153.197119     175.099793     269.137036     217.831335   \n",
      "\n",
      "             Toronto           Essa          Bruce      Southwest  \\\n",
      "count  196919.000000  196919.000000  196919.000000  196919.000000   \n",
      "mean     5793.942164     953.048335      76.546199    3186.886867   \n",
      "min         0.000000     -91.000000    -769.000000       0.000000   \n",
      "25%      4997.000000     796.000000      52.000000    2812.000000   \n",
      "50%      5798.000000     940.000000      74.000000    3171.000000   \n",
      "75%      6474.000000    1092.000000      97.000000    3524.000000   \n",
      "max     10285.000000    1936.000000     734.000000    7342.000000   \n",
      "std      1029.613147     214.961111      34.596890     503.086444   \n",
      "\n",
      "             Niagara           West     Zone Total           Diff  \n",
      "count  196919.000000  196919.000000  196919.000000  196790.000000  \n",
      "mean      537.020618    1701.312347   16145.720520      -0.963850  \n",
      "min     -4255.000000      -6.000000       0.000000   -3284.000000  \n",
      "25%       463.000000    1471.000000   14263.000000     -79.000000  \n",
      "50%       528.000000    1663.000000   16036.000000      -2.000000  \n",
      "75%       600.000000    1902.000000   17882.000000      81.000000  \n",
      "max      1678.000000    3520.000000   26768.000000    9201.000000  \n",
      "std       108.129747     313.314105    2534.781451     139.240907  \n"
     ]
    }
   ],
   "source": [
    "# Combine demandZonal data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = 'rawCSV/demandZonal'\n",
    "output_path = 'rawCSV/combinedDatasets'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Initialize empty list to store dataframes\n",
    "all_data = []\n",
    "\n",
    "# Loop through years 2003-2025\n",
    "for year in range(2003, 2026):\n",
    "    file_path = f'{raw_data_path}/PUB_DemandZonal_{year}.csv'\n",
    "    \n",
    "    try:\n",
    "        # Read CSV, skipping first 3 header rows\n",
    "        df = pd.read_csv(file_path, skiprows=3)\n",
    "        \n",
    "        # Remove any completely empty rows\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Remove any trailing empty columns\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        \n",
    "        all_data.append(df)\n",
    "        print(f\"✓ Successfully loaded {year}: {len(df)} rows\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {year}: {str(e)}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Clean the data\n",
    "    # Remove any rows where Date is empty\n",
    "    combined_df = combined_df.dropna(subset=['Date'], how='all')\n",
    "    \n",
    "    # Convert data types\n",
    "    # Handle mixed date formats (YYYY-MM-DD and YYYY/MM/DD)\n",
    "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='mixed', errors='coerce')\n",
    "    combined_df['Hour'] = pd.to_numeric(combined_df['Hour'], errors='coerce')\n",
    "    \n",
    "    # Convert all zonal demand columns to numeric\n",
    "    numeric_columns = ['Ontario Demand', 'Northwest', 'Northeast', 'Ottawa', 'East', \n",
    "                      'Toronto', 'Essa', 'Bruce', 'Southwest', 'Niagara', 'West', \n",
    "                      'Zone Total', 'Diff']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in combined_df.columns:\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "    \n",
    "    # Remove rows with invalid dates\n",
    "    combined_df = combined_df.dropna(subset=['Date'])\n",
    "    \n",
    "    # Sort by date and hour\n",
    "    combined_df = combined_df.sort_values(['Date', 'Hour']).reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = f'{output_path}/combined_demandZonal_2003_2025.csv'\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Successfully combined {len(all_data)} files\")\n",
    "    print(f\"✓ Total rows: {len(combined_df):,}\")\n",
    "    print(f\"✓ Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "    print(f\"✓ Saved to: {output_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Display first and last few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(combined_df.head())\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(combined_df.tail())\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nData Info:\")\n",
    "    print(combined_df.info())\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(combined_df.describe())\n",
    "    \n",
    "else:\n",
    "    print(\"No data files were successfully loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c83393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2015.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2016.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2017.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2018.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2019.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2020.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2021.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2022.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2023.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2024.xml...\n",
      "Processing rawCSV/GenOutputbyFuelHourly/PUB_GenOutputbyFuelHourly_2025.xml...\n",
      "\n",
      "Creating DataFrame with 94608 rows...\n",
      "\n",
      "DataFrame shape: (94608, 9)\n",
      "\n",
      "Columns: ['Date', 'Hour', 'NUCLEAR', 'GAS', 'HYDRO', 'WIND', 'SOLAR', 'BIOFUEL', 'OTHER']\n",
      "\n",
      "First few rows:\n",
      "         Date  Hour  NUCLEAR   GAS  HYDRO  WIND  SOLAR  BIOFUEL  OTHER\n",
      "0  2015-01-01     1    11564   957   3173  2504    0.0       20    NaN\n",
      "1  2015-01-01    10    11270   953   3932  2605    0.0       20    NaN\n",
      "2  2015-01-01    11    11416   951   3767  2704    0.0       20    NaN\n",
      "3  2015-01-01    12    11570   954   3719  2444    0.0       20    NaN\n",
      "4  2015-01-01    13    11562   952   3826  2741    0.0       20    NaN\n",
      "5  2015-01-01    14    11560   951   3974  2733    0.0       20    NaN\n",
      "6  2015-01-01    15    11565   970   3947  2586    0.0       20    NaN\n",
      "7  2015-01-01    16    11564  1007   4202  2266    0.0       20    NaN\n",
      "8  2015-01-01    17    11564  1093   4296  2634    0.0       19    NaN\n",
      "9  2015-01-01    18    11564  1083   5056  2760    0.0       18    NaN\n",
      "\n",
      "Last few rows:\n",
      "             Date  Hour  NUCLEAR   GAS  HYDRO  WIND  SOLAR  BIOFUEL  OTHER\n",
      "94598  2025-10-16    22     8202  4277   3311   360    0.0       66   81.0\n",
      "94599  2025-10-16    23     8200  4164   3223   408    0.0       54   11.0\n",
      "94600  2025-10-16    24     8198  4024   3168   339    0.0       51    0.0\n",
      "94601  2025-10-16     3     8188  2549   2853   870    0.0       60    0.0\n",
      "94602  2025-10-16     4     8187  2739   2697   898    0.0       56    1.0\n",
      "94603  2025-10-16     5     8195  2939   2867   840    0.0       63    1.0\n",
      "94604  2025-10-16     6     8188  3548   3274   867    0.0       66    0.0\n",
      "94605  2025-10-16     7     8188  4222   3415   711    4.0      124   83.0\n",
      "94606  2025-10-16     8     8189  4341   3195   538  100.0      110   82.0\n",
      "94607  2025-10-16     9     8194  4166   3137   274  241.0      104    2.0\n",
      "\n",
      "Data types:\n",
      "Date        object\n",
      "Hour         int64\n",
      "NUCLEAR      int64\n",
      "GAS          int64\n",
      "HYDRO        int64\n",
      "WIND         int64\n",
      "SOLAR      float64\n",
      "BIOFUEL      int64\n",
      "OTHER      float64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Date           0\n",
      "Hour           0\n",
      "NUCLEAR        0\n",
      "GAS            0\n",
      "HYDRO          0\n",
      "WIND           0\n",
      "SOLAR          9\n",
      "BIOFUEL        0\n",
      "OTHER      89874\n",
      "dtype: int64\n",
      "\n",
      "Data successfully exported to combined_generation_data.csv\n"
     ]
    }
   ],
   "source": [
    "# combine gen Output by Fuel hourly\n",
    "\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define the directory containing XML files\n",
    "data_dir = 'rawCSV/GenOutputbyFuelHourly'\n",
    "\n",
    "# Initialize list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Loop through years 2015-2025\n",
    "for year in range(2015, 2026):\n",
    "    file_path = os.path.join(data_dir, f'PUB_GenOutputbyFuelHourly_{year}.xml')\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: {file_path} not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Define namespace (based on the XML structure)\n",
    "        namespace = {'ns': 'http://www.ieso.ca/schema'}\n",
    "        \n",
    "        # Find all DailyData elements\n",
    "        for daily_data in root.findall('.//ns:DailyData', namespace):\n",
    "            # Get the day\n",
    "            day_elem = daily_data.find('ns:Day', namespace)\n",
    "            day = day_elem.text if day_elem is not None else None\n",
    "            \n",
    "            # Find all HourlyData elements\n",
    "            for hourly_data in daily_data.findall('ns:HourlyData', namespace):\n",
    "                # Get the hour\n",
    "                hour_elem = hourly_data.find('ns:Hour', namespace)\n",
    "                hour = hour_elem.text if hour_elem is not None else None\n",
    "                \n",
    "                # Create a row dict with day and hour\n",
    "                row_data = {\n",
    "                    'Date': day,\n",
    "                    'Hour': hour\n",
    "                }\n",
    "                \n",
    "                # Find all FuelTotal elements\n",
    "                for fuel_total in hourly_data.findall('ns:FuelTotal', namespace):\n",
    "                    # Get fuel type\n",
    "                    fuel_elem = fuel_total.find('ns:Fuel', namespace)\n",
    "                    fuel_type = fuel_elem.text if fuel_elem is not None else None\n",
    "                    \n",
    "                    # Get output value\n",
    "                    output_elem = fuel_total.find('.//ns:Output', namespace)\n",
    "                    output_value = output_elem.text if output_elem is not None else None\n",
    "                    \n",
    "                    # Add fuel type as column\n",
    "                    if fuel_type and output_value:\n",
    "                        row_data[fuel_type] = int(output_value)\n",
    "                \n",
    "                # Append row to all_data\n",
    "                all_data.append(row_data)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Create DataFrame from all collected data\n",
    "print(f\"\\nCreating DataFrame with {len(all_data)} rows...\")\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Sort by Date and Hour\n",
    "df = df.sort_values(['Date', 'Hour']).reset_index(drop=True)\n",
    "\n",
    "# Convert Hour to integer\n",
    "df['Hour'] = df['Hour'].astype(int)\n",
    "\n",
    "# Display info\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nLast few rows:\")\n",
    "print(df.tail(10))\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'combined_generation_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nData successfully exported to {output_file}\")\n",
    "\n",
    "\n",
    "# clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d781a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 rows (should show hours 1-24 for Jan 1):\n",
      "         Date  Hour            DateTime  NUCLEAR\n",
      "0  2015-01-01     1 2015-01-01 01:00:00    11564\n",
      "1  2015-01-01     2 2015-01-01 02:00:00    11560\n",
      "2  2015-01-01     3 2015-01-01 03:00:00    11560\n",
      "3  2015-01-01     4 2015-01-01 04:00:00    11444\n",
      "4  2015-01-01     5 2015-01-01 05:00:00    11174\n",
      "5  2015-01-01     6 2015-01-01 06:00:00    10974\n",
      "6  2015-01-01     7 2015-01-01 07:00:00    10976\n",
      "7  2015-01-01     8 2015-01-01 08:00:00    11199\n",
      "8  2015-01-01     9 2015-01-01 09:00:00    11271\n",
      "9  2015-01-01    10 2015-01-01 10:00:00    11270\n",
      "10 2015-01-01    11 2015-01-01 11:00:00    11416\n",
      "11 2015-01-01    12 2015-01-01 12:00:00    11570\n",
      "12 2015-01-01    13 2015-01-01 13:00:00    11562\n",
      "13 2015-01-01    14 2015-01-01 14:00:00    11560\n",
      "14 2015-01-01    15 2015-01-01 15:00:00    11565\n",
      "15 2015-01-01    16 2015-01-01 16:00:00    11564\n",
      "16 2015-01-01    17 2015-01-01 17:00:00    11564\n",
      "17 2015-01-01    18 2015-01-01 18:00:00    11564\n",
      "18 2015-01-01    19 2015-01-01 19:00:00    11569\n",
      "19 2015-01-01    20 2015-01-01 20:00:00    11567\n",
      "20 2015-01-01    21 2015-01-01 21:00:00    11567\n",
      "21 2015-01-01    22 2015-01-01 22:00:00    11562\n",
      "22 2015-01-01    23 2015-01-01 23:00:00    11561\n",
      "23 2015-01-01    24 2015-01-02 00:00:00    11558\n",
      "24 2015-01-02     1 2015-01-02 01:00:00    11566\n",
      "25 2015-01-02     2 2015-01-02 02:00:00    11566\n",
      "26 2015-01-02     3 2015-01-02 03:00:00    11566\n",
      "27 2015-01-02     4 2015-01-02 04:00:00    11560\n",
      "28 2015-01-02     5 2015-01-02 05:00:00    11561\n",
      "29 2015-01-02     6 2015-01-02 06:00:00    11561\n",
      "\n",
      "Hour 24 examples (should show next day DateTime):\n",
      "          Date  Hour   DateTime  Hour_Normalized\n",
      "23  2015-01-01    24 2015-01-02                0\n",
      "47  2015-01-02    24 2015-01-03                0\n",
      "71  2015-01-03    24 2015-01-04                0\n",
      "95  2015-01-04    24 2015-01-05                0\n",
      "119 2015-01-05    24 2015-01-06                0\n",
      "\n",
      "Total rows with Hour 24: 3942\n",
      "\n",
      "✅ Corrected data saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('combined_generation_data.csv')\n",
    "\n",
    "# Convert Date to datetime and ensure Hour is integer\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Hour'] = df['Hour'].astype(int)\n",
    "\n",
    "# Re-sort properly\n",
    "df = df.sort_values(['Date', 'Hour']).reset_index(drop=True)\n",
    "\n",
    "# FIX FOR HOUR 24: Handle hour 24 as hour 0 of the next day\n",
    "# Create a temporary copy for datetime calculation\n",
    "df_temp = df.copy()\n",
    "\n",
    "# For rows where Hour == 24, convert to Hour 0 and add 1 day to the date\n",
    "mask = df_temp['Hour'] == 24\n",
    "df_temp.loc[mask, 'Date'] = df_temp.loc[mask, 'Date'] + pd.Timedelta(days=1)\n",
    "df_temp.loc[mask, 'Hour'] = 0\n",
    "\n",
    "# Create DateTime column using the adjusted values\n",
    "df['DateTime'] = pd.to_datetime(\n",
    "    df_temp['Date'].astype(str) + ' ' + \n",
    "    df_temp['Hour'].astype(str).str.zfill(2) + ':00:00'\n",
    ")\n",
    "\n",
    "# Keep original Hour column (1-24) but add normalized version (0-23)\n",
    "df['Hour_Normalized'] = df['Hour'].apply(lambda x: 0 if x == 24 else x)\n",
    "\n",
    "# Verify the fix worked\n",
    "print(\"First 30 rows (should show hours 1-24 for Jan 1):\")\n",
    "print(df.head(30)[['Date', 'Hour', 'DateTime', 'NUCLEAR']])\n",
    "\n",
    "print(\"\\nHour 24 examples (should show next day DateTime):\")\n",
    "print(df[df['Hour'] == 24].head(5)[['Date', 'Hour', 'DateTime', 'Hour_Normalized']])\n",
    "\n",
    "print(f\"\\nTotal rows with Hour 24: {(df['Hour'] == 24).sum()}\")\n",
    "\n",
    "# Save corrected version\n",
    "df.to_csv('combined_generation_data_corrected.csv', index=False)\n",
    "print(\"\\n✅ Corrected data saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f52b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset: 205680 rows, 23 columns\n"
     ]
    }
   ],
   "source": [
    "# make master dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the three datasets\n",
    "df1 = pd.read_csv('rawCSV/combinedDatasets/combined_demand_2002_2025.csv')\n",
    "df2 = pd.read_csv('rawCSV/combinedDatasets/combined_demandZonal_2003_2025.csv')\n",
    "df3 = pd.read_csv('rawCSV/combinedDatasets/combined_generation_data_corrected.csv')\n",
    "\n",
    "# Step 1: Merge demand and zonal data\n",
    "# Drop duplicate 'Ontario Demand' column from df2 to avoid conflicts\n",
    "merged_df = pd.merge(\n",
    "    df1, \n",
    "    df2.drop(columns=['Ontario Demand']),\n",
    "    on=['Date', 'Hour'], \n",
    "    how='outer',\n",
    "    suffixes=('', '_zonal')\n",
    ")\n",
    "\n",
    "# Step 2: Clean and merge generation data\n",
    "# Remove redundant columns from df3\n",
    "df3_clean = df3.drop(columns=['DateTime', 'Hour_Normalized'], errors='ignore')\n",
    "\n",
    "final_df = pd.merge(\n",
    "    merged_df,\n",
    "    df3_clean,\n",
    "    on=['Date', 'Hour'],\n",
    "    how='left',  # Left join keeps all dates; generation columns will be NaN pre-2015\n",
    "    suffixes=('', '_gen')\n",
    ")\n",
    "\n",
    "# Save combined dataset\n",
    "final_df.to_csv('combined_complete_dataset.csv', index=False)\n",
    "print(f\"Combined dataset: {final_df.shape[0]} rows, {final_df.shape[1]} columns\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
