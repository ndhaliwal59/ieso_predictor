{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7849d0",
   "metadata": {},
   "source": [
    "running on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59358d3d",
   "metadata": {},
   "source": [
    "# Baseline TFT for Ontario Demand (Kaggle)\n",
    "# - Target: \"Ontario Demand\"\n",
    "# - Drops \"Market Demand\" entirely to avoid leakage\n",
    "# - Minimal known future covariates: hour, day_of_week, month, time_idx\n",
    "# - Encoder: 168h (7 days), Prediction length: 24h\n",
    "\n",
    "# 0) Installs (safe on Kaggle)\n",
    "!pip -q install \"pytorch-forecasting==1.4.0\" \"lightning>=2.2,<2.5\" \"torchmetrics>=1.3,<1.5\" --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126c7fa",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 1) Imports and seed\n",
    "# ==============================\n",
    "import os, glob, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# ==============================\n",
    "# 2) Locate and inspect data\n",
    "# ==============================\n",
    "base_dir = \"/kaggle/input/iess-demand-2002-2025\"\n",
    "csv_candidates = []\n",
    "for ext in (\"*.csv\", \"*.CSV\"):\n",
    "    csv_candidates.extend(glob.glob(os.path.join(base_dir, \"**\", ext), recursive=True))\n",
    "assert len(csv_candidates) > 0, \"No CSV files found under /kaggle/input/iess-demand-2002-2025\"\n",
    "csv_path = max(csv_candidates, key=os.path.getsize)\n",
    "print(f\"Using file: {csv_path}\")\n",
    "\n",
    "# Optional: show available inputs\n",
    "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "    for filename in filenames[:5]:\n",
    "        print(os.path.join(dirname, filename))\n",
    "    break\n",
    "\n",
    "# ==============================\n",
    "# 3) Load & preprocess\n",
    "# ==============================\n",
    "df = pd.read_csv(csv_path)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Ensure target exists\n",
    "assert \"Ontario Demand\" in df.columns, \"Expected 'Ontario Demand' column\"\n",
    "\n",
    "# Drop Market Demand to avoid leakage in baseline\n",
    "df = df.drop(columns=[\"Market Demand\"], errors=\"ignore\")\n",
    "\n",
    "# Parse datetime from Date + Hour (Hour is 1..24)\n",
    "df[\"Hour\"] = pd.to_numeric(df[\"Hour\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Date\", \"Hour\"]).copy()\n",
    "df[\"Hour\"] = df[\"Hour\"].astype(int).clip(1, 24)\n",
    "\n",
    "# Build hourly timestamp mapping 1..24 -> 0..23 per day\n",
    "df[\"time\"] = df[\"Date\"] + pd.to_timedelta(df[\"Hour\"] - 1, unit=\"h\")\n",
    "df[\"Ontario Demand\"] = pd.to_numeric(df[\"Ontario Demand\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Ontario Demand\", \"time\"]).sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "# Make series id and enforce continuous hourly grid\n",
    "df = df.drop_duplicates(subset=[\"time\"], keep=\"first\").sort_values(\"time\")\n",
    "full_range = pd.date_range(df[\"time\"].min(), df[\"time\"].max(), freq=\"h\")  # use 'h'\n",
    "df = df.set_index(\"time\").reindex(full_range).rename_axis(\"time\").reset_index()\n",
    "\n",
    "# Forward-fill target gaps created by reindexing (baseline)\n",
    "df[\"Ontario Demand\"] = df[\"Ontario Demand\"].astype(float).ffill()\n",
    "\n",
    "# Single series id\n",
    "df[\"series\"] = \"ON\"\n",
    "\n",
    "# Continuous integer time index in hours\n",
    "df[\"time_idx\"] = ((df[\"time\"] - df[\"time\"].min()).dt.total_seconds() // 3600).astype(int)\n",
    "\n",
    "# Minimal calendar covariates\n",
    "df[\"hour\"] = df[\"time\"].dt.hour.astype(\"int16\")\n",
    "df[\"day_of_week\"] = df[\"time\"].dt.dayofweek.astype(\"int8\")\n",
    "df[\"month\"] = df[\"time\"].dt.month.astype(\"int8\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# ==============================\n",
    "# 4) Split and dataset\n",
    "# ==============================\n",
    "max_encoder_length = 168   # 7 days\n",
    "max_prediction_length = 24 # next 24 hours\n",
    "\n",
    "# Chronological split: last 7 days for validation\n",
    "cutoff = df[\"time_idx\"].max() - max_prediction_length * 7\n",
    "training_df = df[df[\"time_idx\"] <= cutoff].copy()\n",
    "validation_df = df[df[\"time_idx\"] > cutoff].copy()\n",
    "assert len(training_df) > 0 and len(validation_df) > 0\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    training_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Ontario Demand\",\n",
    "    group_ids=[\"series\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "\n",
    "    # Minimal baseline features\n",
    "    time_varying_known_reals=[\"time_idx\", \"hour\", \"day_of_week\", \"month\"],\n",
    "    time_varying_unknown_reals=[\"Ontario Demand\"],\n",
    "\n",
    "    # Normalization and helpers\n",
    "    target_normalizer=GroupNormalizer(groups=[\"series\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "\n",
    "    # Tolerate any residual gaps (safety)\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, df, predict=True, stop_randomization=True\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 5) Dataloaders\n",
    "# ==============================\n",
    "batch_size = 128\n",
    "train_loader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0, persistent_workers=False\n",
    ")\n",
    "val_loader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 2, num_workers=0, persistent_workers=False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 6) Model and trainer\n",
    "# ==============================\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=1e-3,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    loss=QuantileLoss(),      # robust multi-horizon baseline\n",
    "    optimizer=\"adam\",\n",
    "    reduce_on_plateau_patience=3,\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,             # quick baseline\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7) Train\n",
    "# ==============================\n",
    "trainer.fit(tft, train_loader, val_loader)\n",
    "\n",
    "# ==============================\n",
    "# 8) Validation predictions & metrics (fixed)\n",
    "# ==============================\n",
    "# Predicted 24-step horizons for each validation window (median for QuantileLoss)\n",
    "preds = tft.predict(val_loader)  # shape [N_windows, 24] for single target\n",
    "\n",
    "# Collect actual targets from val_loader by unpacking y=(target, weight)\n",
    "actuals_list = []\n",
    "for _, y in val_loader:\n",
    "    # y is (target, weight). target may be a list if multiple targets.\n",
    "    if isinstance(y, (list, tuple)):\n",
    "        target = y[0]\n",
    "    else:\n",
    "        target = y\n",
    "    if isinstance(target, list):  # multi-target case\n",
    "        target = target[0]\n",
    "    actuals_list.append(target)\n",
    "\n",
    "actuals = torch.cat([t.detach().cpu().float() for t in actuals_list], dim=0)\n",
    "\n",
    "# Convert preds to CPU float\n",
    "preds = preds.detach().cpu().float()\n",
    "\n",
    "# Overall metrics\n",
    "mae = torch.mean(torch.abs(actuals - preds)).item()\n",
    "rmse = torch.sqrt(torch.mean((actuals - preds) ** 2)).item()\n",
    "smape = torch.mean(200.0 * torch.abs(actuals - preds) / (torch.abs(actuals) + torch.abs(preds) + 1e-6)).item()\n",
    "print(f\"Validation MAE:  {mae:.2f}\")\n",
    "print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "print(f\"Validation sMAPE: {smape:.2f}%\")\n",
    "\n",
    "# Per-horizon metrics (1..24)\n",
    "per_h_mae = torch.mean(torch.abs(actuals - preds), dim=0).numpy()\n",
    "per_h_rmse = torch.sqrt(torch.mean((actuals - preds) ** 2, dim=0)).numpy()\n",
    "per_h_smape = torch.mean(200.0 * torch.abs(actuals - preds) / (torch.abs(actuals) + torch.abs(preds) + 1e-6), dim=0).numpy()\n",
    "\n",
    "per_h_df = pd.DataFrame({\n",
    "    \"horizon_hour_ahead\": np.arange(1, preds.shape[1] + 1),\n",
    "    \"MAE\": per_h_mae,\n",
    "    \"RMSE\": per_h_rmse,\n",
    "    \"sMAPE_%\": per_h_smape,\n",
    "})\n",
    "print(per_h_df.head(10))\n",
    "per_h_df.to_csv(\"/kaggle/working/validation_per_horizon_metrics.csv\", index=False)\n",
    "\n",
    "# Optional: inspect first validation window\n",
    "sample_idx = 0\n",
    "inspect_df = pd.DataFrame({\n",
    "    \"horizon_hour_ahead\": np.arange(1, preds.shape[1] + 1),\n",
    "    \"pred_p50\": preds[sample_idx].numpy(),\n",
    "    \"actual\": actuals[sample_idx].numpy(),\n",
    "})\n",
    "print(inspect_df.head(12))\n",
    "inspect_df.to_csv(\"/kaggle/working/sample_validation_window.csv\", index=False)\n",
    "\n",
    "print(\"Artifacts saved to /kaggle/working/: validation_per_horizon_metrics.csv, sample_validation_window.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0223d9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
