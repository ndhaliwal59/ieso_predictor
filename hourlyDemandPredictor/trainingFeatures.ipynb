{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a18569e",
   "metadata": {},
   "source": [
    "ran on kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357180d",
   "metadata": {},
   "source": [
    "# Baseline TFT for Ontario Demand using engineered parquet\n",
    "# - Target: \"Ontario Demand\"\n",
    "# - Uses precomputed features from /ontario_demand_basic_features.parquet\n",
    "# - Encoder: 168h (7 days), Prediction length: 24h\n",
    "\n",
    "# 0) Installs (safe on Kaggle)\n",
    "!pip -q install \"pytorch-forecasting==1.4.0\" \"lightning>=2.2,<2.5\" \"torchmetrics>=1.3,<1.5\" --no-cache-dir\n",
    " \n",
    "# ==============================\n",
    "# 1) Imports and seed\n",
    "# ==============================\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# ==============================\n",
    "# 2) Load engineered parquet\n",
    "# ==============================\n",
    "# If your Kaggle dataset mounts elsewhere, adjust the path accordingly.\n",
    "parquet_path = \"/kaggle/input/ontario-demand-basic-features\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Clean column names lightly (keep spaces for target)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Sanity checks\n",
    "assert \"Ontario Demand\" in df.columns, \"Expected 'Ontario Demand' column\"\n",
    "assert \"time_idx\" in df.columns, \"Expected 'time_idx' column\"\n",
    "assert \"series_id\" in df.columns, \"Expected 'series_id' column\"\n",
    "\n",
    "# Ensure dtypes/order\n",
    "if \"timestamp\" in df.columns:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values(\"time_idx\").reset_index(drop=True)\n",
    "df[\"Ontario Demand\"] = pd.to_numeric(df[\"Ontario Demand\"], errors=\"coerce\")\n",
    "\n",
    "# ==============================\n",
    "# 3) Feature sets\n",
    "# ==============================\n",
    "# Known in future (calendar + harmonics + flags; adjust as needed)\n",
    "known_reals = [\n",
    "    \"time_idx\",\n",
    "    \"hour\", \"dow\", \"day\", \"week\", \"month\", \"quarter\", \"dayofyear\",\n",
    "    \"is_weekend\", \"is_business_day\", \"is_holiday\", \"is_holiday_eve\", \"is_holiday_morrow\",\n",
    "    \"season_code\", \"is_month_start\", \"is_month_end\", \"is_qtr_end\", \"is_year_end\",\n",
    "    \"is_dst\", \"is_peak_hour\",\n",
    "    \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"doy_sin\", \"doy_cos\",\n",
    "]\n",
    "\n",
    "# Unknown in future (encoder-only observed inputs: target + lags/rolls/ewm)\n",
    "unknown_reals = [\n",
    "    \"Ontario Demand\",\n",
    "    \"Ontario Demand_lag_1h\", \"Ontario Demand_lag_24h\", \"Ontario Demand_lag_168h\",\n",
    "    \"Ontario Demand_rollmean_24h\", \"Ontario Demand_rollstd_24h\",\n",
    "    \"Ontario Demand_rollmean_168h\", \"Ontario Demand_rollstd_168h\",\n",
    "    \"Ontario Demand_ewm_24h\", \"Ontario Demand_ewm_168h\",\n",
    "]\n",
    "\n",
    "# Keep only existing columns (handles cases where some features are missing)\n",
    "known_reals = [c for c in known_reals if c in df.columns]\n",
    "unknown_reals = [c for c in unknown_reals if c in df.columns]\n",
    "\n",
    "# Optional: drop rows with missing target\n",
    "df = df.dropna(subset=[\"Ontario Demand\"]).reset_index(drop=True)\n",
    "\n",
    "# ==============================\n",
    "# 4) Split and dataset\n",
    "# ==============================\n",
    "max_encoder_length = 168   # 7 days\n",
    "max_prediction_length = 24 # next 24 hours\n",
    "\n",
    "# Chronological split: last 7 days for validation\n",
    "cutoff = df[\"time_idx\"].max() - max_prediction_length * 7\n",
    "\n",
    "\n",
    "max_history = 168  # largest lookback among *_lag_168h / *_roll*_168h / *_ewm_168h\n",
    "min_time_per_series = df.groupby(\"series_id\")[\"time_idx\"].transform(\"min\")\n",
    "df = df[df[\"time_idx\"] >= (min_time_per_series + max_history)].copy()\n",
    "df = df.sort_values(\"time_idx\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "training_df = df[df[\"time_idx\"] <= cutoff].copy()\n",
    "validation_df = df[df[\"time_idx\"] > cutoff].copy()\n",
    "assert len(training_df) > 0 and len(validation_df) > 0\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    training_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Ontario Demand\",\n",
    "    group_ids=[\"series_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "\n",
    "    time_varying_known_reals=known_reals,\n",
    "    time_varying_unknown_reals=unknown_reals,\n",
    "\n",
    "    # Normalization and helpers\n",
    "    target_normalizer=GroupNormalizer(groups=[\"series_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "\n",
    "    # Tolerate any residual gaps\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "# Build validation dataset aligned to the training definition\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, df, predict=True, stop_randomization=True\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 5) Dataloaders\n",
    "# ==============================\n",
    "batch_size = 128\n",
    "train_loader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0, persistent_workers=False\n",
    ")\n",
    "val_loader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 2, num_workers=0, persistent_workers=False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 6) Model and trainer\n",
    "# ==============================\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=1e-3,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    loss=QuantileLoss(),      # robust multi-horizon baseline\n",
    "    optimizer=\"adam\",\n",
    "    reduce_on_plateau_patience=3,\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,             # quick baseline\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7) Train\n",
    "# ==============================\n",
    "trainer.fit(tft, train_loader, val_loader)\n",
    "\n",
    "# ==============================\n",
    "# 8) Validation predictions & metrics (unchanged)\n",
    "# ==============================\n",
    "preds = tft.predict(val_loader)  # [N_windows, 24]\n",
    "\n",
    "# Collect actual targets from val_loader by unpacking y=(target, weight)\n",
    "actuals_list = []\n",
    "for _, y in val_loader:\n",
    "    target = y[0] if isinstance(y, (list, tuple)) else y\n",
    "    if isinstance(target, list):\n",
    "        target = target[0]\n",
    "    actuals_list.append(target)\n",
    "\n",
    "actuals = torch.cat([t.detach().cpu().float() for t in actuals_list], dim=0)\n",
    "preds = preds.detach().cpu().float()\n",
    "\n",
    "# Overall metrics\n",
    "mae = torch.mean(torch.abs(actuals - preds)).item()\n",
    "rmse = torch.sqrt(torch.mean((actuals - preds) ** 2)).item()\n",
    "smape = torch.mean(200.0 * torch.abs(actuals - preds) / (torch.abs(actuals) + torch.abs(preds) + 1e-6)).item()\n",
    "print(f\"Validation MAE:  {mae:.2f}\")\n",
    "print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "print(f\"Validation sMAPE: {smape:.2f}%\")\n",
    "\n",
    "# Per-horizon metrics (1..24)\n",
    "per_h_mae = torch.mean(torch.abs(actuals - preds), dim=0).numpy()\n",
    "per_h_rmse = torch.sqrt(torch.mean((actuals - preds) ** 2, dim=0)).numpy()\n",
    "per_h_smape = torch.mean(200.0 * torch.abs(actuals - preds) / (torch.abs(actuals) + torch.abs(preds) + 1e-6), dim=0).numpy()\n",
    "\n",
    "import pandas as pd\n",
    "per_h_df = pd.DataFrame({\n",
    "    \"horizon_hour_ahead\": np.arange(1, preds.shape[1] + 1),\n",
    "    \"MAE\": per_h_mae,\n",
    "    \"RMSE\": per_h_rmse,\n",
    "    \"sMAPE_%\": per_h_smape,\n",
    "})\n",
    "print(per_h_df.head(10))\n",
    "per_h_df.to_csv(\"/kaggle/working/validation_per_horizon_metrics.csv\", index=False)\n",
    "\n",
    "# Optional: inspect first validation window\n",
    "sample_idx = 0\n",
    "inspect_df = pd.DataFrame({\n",
    "    \"horizon_hour_ahead\": np.arange(1, preds.shape[1] + 1),\n",
    "    \"pred_p50\": preds[sample_idx].numpy(),\n",
    "    \"actual\": actuals[sample_idx].numpy(),\n",
    "})\n",
    "print(inspect_df.head(12))\n",
    "inspect_df.to_csv(\"/kaggle/working/sample_validation_window.csv\", index=False)\n",
    "\n",
    "print(\"Artifacts saved to /kaggle/working/: validation_per_horizon_metrics.csv, sample_validation_window.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3725d1",
   "metadata": {},
   "source": [
    "Validation MAE:  558.37\n",
    "Validation RMSE: 636.15\n",
    "Validation sMAPE: 3.82%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
